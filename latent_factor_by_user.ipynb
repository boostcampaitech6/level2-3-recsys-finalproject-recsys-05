{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orjson\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import orjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tier = 'diamond'\n",
    "output_dir = \"/home/piddle/hdd/matches\"\n",
    "target_dir = os.path.join(output_dir, tier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(target_dir, os.listdir(target_dir)[6])\n",
    "file = orjson.loads(open(path, 'rb').read())\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_column=['match_id', 'game_length_second', 'summoner_id', 'summoner_level', 'champion_id', 'team_key', 'position',\n",
    "             'trinket_item', 'item_0', 'item_1', 'item_2', 'item_3', 'item_4', 'rune_0', 'rune_1', 'spell_0', 'spell_1',\n",
    "             'champion_level', 'damage_self_mitigated', 'damage_dealt_to_objectives', 'damage_dealt_to_turrets',\n",
    "             'total_damage_taken', 'total_damage_dealt', 'total_damage_dealt_to_champions', 'time_ccing_others',\n",
    "             'vision_score', 'vision_wards_bought_in_game', 'sight_wards_bought_in_game', 'ward_kill', 'ward_place',\n",
    "             'turret_kill', 'kill', 'death', 'assist', 'neutral_minion_kill', 'gold_earned', 'total_heal', 'result']\n",
    "    \n",
    "cols1 = ['champion_id', 'team_key', 'position', 'trinket_item']\n",
    "\n",
    "stat_cols = ['champion_level', 'damage_self_mitigated', 'damage_dealt_to_objectives', 'damage_dealt_to_turrets',\n",
    "            'total_damage_taken', 'total_damage_dealt', 'total_damage_dealt_to_champions', 'time_ccing_others',\n",
    "            'time_ccing_others', 'vision_wards_bought_in_game', 'sight_wards_bought_in_game', 'ward_kill', 'ward_place',\n",
    "            'turret_kill', 'kill', 'death', 'assist', 'neutral_minion_kill', 'gold_earned', 'total_heal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_match(file_iter) -> pd.DataFrame:\n",
    "    data_chunk = []\n",
    "    for n, file in enumerate(tqdm(file_iter)):\n",
    "        with open(file.path, 'rb') as f:\n",
    "            try:\n",
    "                json_data = orjson.loads(f.read())\n",
    "            except Exception as e:\n",
    "                print('Error reading {}: {}'.format(file.path, e))\n",
    "                continue\n",
    "\n",
    "        for match in json_data:\n",
    "            for participant in match['participants']:\n",
    "                data= {}\n",
    "                data['match_id'] = match['id']\n",
    "                data['game_length_second'] = match['game_length_second']\n",
    "\n",
    "                data['summoner_id'] = participant['summoner']['summoner_id']\n",
    "                data['summoner_level'] = participant['summoner']['level']\n",
    "\n",
    "                for col in cols1:\n",
    "                    data[col] = participant[col]\n",
    "\n",
    "                for i, item in enumerate(participant['items']):\n",
    "                    data[f'item_{i}'] = item\n",
    "\n",
    "                data['rune_0'] = participant['rune'][\"primary_rune_id\"]\n",
    "                data['rune_1'] = participant['rune'][\"secondary_page_id\"]\n",
    "                data['spell_0'] = participant['spells'][0]\n",
    "                data['spell_1'] = participant['spells'][1]\n",
    "\n",
    "                stats = participant['stats']\n",
    "                for col in stat_cols:\n",
    "                    data[col] = stats[col] / match['game_length_second']\n",
    "\n",
    "                data['vision_score'] = stats['vision_score']\n",
    "                data['result'] = stats['result']\n",
    "\n",
    "                data_chunk.append(data)\n",
    "\n",
    "        if n % 20000 == 0:\n",
    "            df = pd.DataFrame(data_chunk, columns=all_column)\n",
    "            data_chunk = []\n",
    "            yield df\n",
    "\n",
    "    else:\n",
    "        df = pd.DataFrame(data_chunk, columns=all_column)\n",
    "        yield df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_iter = os.scandir(target_dir)\n",
    "participants = pd.DataFrame(columns=all_column)\n",
    "participants.to_csv(f'../data/{tier}_matches.csv', mode='w', index=False)\n",
    "\n",
    "for df in parse_match(file_iter):\n",
    "    pass\n",
    "    df.to_csv(f'../data/{tier}_matches.csv', mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'../data/{tier}_matches.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.combinations = [list(element) for element in itertools.combinations(range(5), 2)]\n",
    "        self.combinations_len = len(self.combinations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        df_idx = idx // self.combinations_len\n",
    "        combination_idx = idx % self.combinations_len\n",
    "\n",
    "        x = self.df.iloc[df_idx, self.combinations[combination_idx]]\n",
    "        x = torch.tensor(x, dtype=torch.int32, device='cuda')\n",
    "\n",
    "        y = self.df.iloc[df_idx, -1]\n",
    "        y = torch.tensor(y, dtype=torch.float64, device='cuda')\n",
    "\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.combinations) * len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SimilarityModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(config['n_layers'], config['emb_size'])\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.transpose_(0, 1)\n",
    "        embedded = self.embedding(input)\n",
    "        A = embedded[0]\n",
    "        B = embedded[1]\n",
    "        output = F.cosine_similarity(A, B, dim=1)     \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2521284 [00:00<?, ?it/s]/tmp/ipykernel_34020/378348694.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x = torch.tensor(x, dtype=torch.int32, device='cuda')\n",
      "100%|██████████| 2521284/2521284 [2:06:43<00:00, 331.59it/s]  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'../data/{tier}_matches.csv')\n",
    "\n",
    "config = {}\n",
    "config['n_layers'] = int(df.drop(columns='result').max().max() + 1)\n",
    "config['emb_size'] = 3\n",
    "\n",
    "dataset = MyDataset(df)\n",
    "loader = DataLoader(dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "model = SimilarityModel(config).to('cuda')\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for x, y in tqdm(loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "\n",
    "    # print(output, y)\n",
    "    loss = loss_fun(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cossim.joblib']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'cossim.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

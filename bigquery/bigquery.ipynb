{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-cloud-bigquery pandas db-dtypes pandas-gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas_gbq\n",
    "import pandas as pd\n",
    "# import db_dtypes\n",
    "# BigQuery 클라이언트 객체를 생성합니다.\n",
    "\n",
    "client = bigquery.Client.from_service_account_json('../../../teemo-415918-c7066e71ebbb.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'teemo-415918'\n",
    "origin_table_id = \"teemo-415918.match_dataset.match_test\"\n",
    "target_table_id = \"teemo-415918.match_dataset.match_v1\"\n",
    "temp_table_id = \"teemo-415918.match_dataset.temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_cols = {}\n",
    "cate_cols['other'] = ['summonerId', 'teamId', 'individualPosition', 'role', 'championId', 'win', 'defense', ' flex', 'offense', 'matchId' ]\n",
    "cate_cols['item'] = ['item0', 'item1', 'item2', 'item3', 'item4', 'item5', 'item6']\n",
    "cate_cols['summonerSpell'] = ['summoner1Id', 'summoner2Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cate in cate_cols.keys():\n",
    "    if cate == 'other':\n",
    "        for col in cate_cols[cate]:\n",
    "            query = f\"\"\"\n",
    "            SELECT `{col}`\n",
    "            FROM `{origin_table_id}`\n",
    "            \"\"\"\n",
    "            df = client.query(query).to_dataframe()\n",
    "\n",
    "            category_mapping = {category: idx for idx, category in enumerate(df[col].unique(), start=1)}\n",
    "\n",
    "            df[col] = df[col].map(category_mapping)\n",
    "\n",
    "            pandas_gbq.to_gbq(df, temp_table_id, project_id=project_id, if_exists='replace')\n",
    "\n",
    "            update_query = f\"\"\"\n",
    "            UPDATE `{target_table_id}` AS target\n",
    "            SET target.{col} = temp.{col}\n",
    "            FROM `{temp_table_id}` AS temp\n",
    "            WHERE target.id = temp.id\n",
    "            \"\"\"\n",
    "\n",
    "    else:\n",
    "        merge_df = pd.DataFrame(columns=[cate])\n",
    "        for col in cate_cols[cate]:\n",
    "            query = f\"\"\"\n",
    "            SELECT `{col}`\n",
    "            FROM `{origin_table_id}`\n",
    "            \"\"\"\n",
    "\n",
    "            df = client.query(query).to_dataframe()\n",
    "            df.columns =[cate]\n",
    "\n",
    "            merge_df = pd.concat([merge_df, df], axis=0)\n",
    "\n",
    "        category_mapping = {category: idx for idx, category in enumerate(merge_df[cate].unique(), start=1)}\n",
    "\n",
    "        for col in cate_cols[cate]:\n",
    "            df[col] = df[col].map(category_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_table = client.get_table(origin_table_id)\n",
    "category_to_index = {}\n",
    "\n",
    "write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "\n",
    "# 총 데이터 수 계산\n",
    "query_count = f\"SELECT COUNT(*) as total FROM `{origin_table_id}`\"\n",
    "total_rows = client.query(query_count).to_dataframe().iloc[0]['total']\n",
    "\n",
    "print('total_rows : ', total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_cols = {}\n",
    "cate_cols['other'] = ['summonerId', 'teamId', 'individualPosition', 'role', 'championId', 'win', 'defense', ' flex', 'offense', 'matchId' ]\n",
    "cate_cols['item'] = ['item0', 'item1', 'item2', 'item3', 'item4', 'item5', 'item6']\n",
    "cate_cols['summonerSpell'] = ['summoner1Id', 'summoner2Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(table_id, col_name, field_type):\n",
    "    schema = [ bigquery.SchemaField(col_name, field_type) ]\n",
    "    table = bigquery.Table(table_id, schema=schema)\n",
    "    client.delete_table(table, not_found_ok=True)\n",
    "    client.create_table(table)\n",
    "\n",
    "\n",
    "def get_unique(col_name):\n",
    "    # 고유 카테고리 값 수집\n",
    "    query_unique_categories = f\"\"\"\n",
    "    SELECT DISTINCT {col_name}\n",
    "    FROM `{origin_table_id}`\n",
    "    \"\"\"\n",
    "\n",
    "    df_unique_categories = client.query(query_unique_categories).to_dataframe()\n",
    "\n",
    "    return df_unique_categories\n",
    "\n",
    "\n",
    "def other_cate(cate_group, batch_size):\n",
    "    num_batches = (total_rows + batch_size - 1) // batch_size\n",
    "    for col_name in cate_cols[cate_group]:\n",
    "        df_unique_categories = get_unique(col_name)\n",
    "        category_to_index[col_name] = {category: idx for idx, category in enumerate(df_unique_categories[col_name], start=1)}\n",
    "\n",
    "        # 인덱싱 테이블을 저장\n",
    "        print('start: ', col_name, \"index_table\")\n",
    "        table_id = pp_table_id + f'.{col_name}_index_table'\n",
    "\n",
    "        for i in origin_table.schema:\n",
    "            if i.name == col_name:\n",
    "                field_type = i.field_type\n",
    "                break\n",
    "\n",
    "        schema = create_table(table_id, col_name, field_type)\n",
    "        \n",
    "        for batch_num in range(num_batches):\n",
    "            start_row = batch_num * batch_size\n",
    "            \n",
    "            cut_df = df_unique_categories.iloc[start_row : start_row + batch_size]\n",
    "\n",
    "            job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=write_disposition)\n",
    "            job = client.load_table_from_dataframe(cut_df, table_id, job_config=job_config)\n",
    "            job.result()\n",
    "\n",
    "\n",
    "        ### 인덱싱된 데이터 저장\n",
    "        print('start: ', col_name, \"indexed_table\")\n",
    "        table_id = pp_table_id + f'.{col_name}_indexed_table'\n",
    "        schema = create_table(table_id, col_name, \"INTEGER\")\n",
    "        for batch_num in range(num_batches):\n",
    "            start_row = batch_num * batch_size\n",
    "            query_batch = f\"\"\"\n",
    "            SELECT {col_name}\n",
    "            FROM `{table_id}`\n",
    "            LIMIT {batch_size} OFFSET {start_row}\n",
    "            \"\"\"\n",
    "            df_batch = client.query(query_batch).to_dataframe()\n",
    "            \n",
    "            # 인덱스 매핑 적용\n",
    "            df_batch[col_name] = df_batch[col_name].map(category_to_index[col_name])\n",
    "            \n",
    "            job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=write_disposition)\n",
    "            job = client.load_table_from_dataframe(df_batch, table_id, job_config=job_config)\n",
    "            job.result()\n",
    "\n",
    "\n",
    "def series_cate(cate_group, batch_size):\n",
    "    num_batches = (total_rows + batch_size - 1) // batch_size\n",
    "    df_unique_categories = pd.DataFrame()\n",
    "    for col_name in cate_cols[cate_group]:\n",
    "        df_unique_categories = pd.concat([df_unique_categories, get_unique(col_name)], axis=0)\n",
    "\n",
    "    print(df_unique_categories.sample(10))\n",
    "    df_unique_categories.drop_duplicates(inplace=True)\n",
    "    category_to_index = {category: idx for idx, category in enumerate(df_unique_categories, start=1)}\n",
    "    \n",
    "    print('start: ', cate_group, \"index_table\")\n",
    "    table_id = pp_table_id + f'.{cate_group}_indexed_table'\n",
    "    schema = create_table(table_id, cate_group, \"INTEGER\")\n",
    "\n",
    "    for batch_num in range(num_batches):\n",
    "        start_row = batch_num * batch_size\n",
    "        \n",
    "        cut_df = df_unique_categories.iloc[start_row : start_row + batch_size]\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=write_disposition)\n",
    "        job = client.load_table_from_dataframe(cut_df, table_id, job_config=job_config)\n",
    "        job.result()\n",
    "\n",
    "    print('start: ', cate_group, \"indexed_table\")\n",
    "    table_id = pp_table_id + f'.{cate_group}_indexed_table'\n",
    "    schema = create_table(table_id, cate_group, \"INTEGER\")\n",
    "    for batch_num in range(num_batches):\n",
    "        start_row = batch_num * batch_size\n",
    "        query_batch = f\"\"\"\n",
    "        SELECT {col_name}\n",
    "        FROM `{table_id}`\n",
    "        LIMIT {batch_size} OFFSET {start_row}\n",
    "        \"\"\"\n",
    "        df_batch = client.query(query_batch).to_dataframe()\n",
    "        \n",
    "        # 인덱스 매핑 적용\n",
    "        df_batch[col_name] = df_batch[col_name].map(category_to_index)\n",
    "        \n",
    "        job_config = bigquery.LoadJobConfig(schema=schema, write_disposition=write_disposition)\n",
    "        job = client.load_table_from_dataframe(df_batch, table_id, job_config=job_config)\n",
    "        job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 처리를 위한 설정\n",
    "batch_size = 100000  # 배치 크기 설정\n",
    "num_batches = (total_rows + batch_size - 1) // batch_size  # 필요한 배치 수 계산\n",
    "\n",
    "for cate_group in cate_cols.keys():\n",
    "    if cate_group == 'other':\n",
    "        other_cate(cate_group, batch_size)\n",
    "\n",
    "    else:\n",
    "        series_cate(cate_group, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cols = []\n",
    "cate_cols = ['summonerId', 'teamId', 'individualPosition', 'role', 'championId', 'item0', 'item1', 'item2', 'item3', 'item4', 'item5', 'item6', \n",
    "             'summoner1Id', 'summoner2Id', 'win', 'defense', ' flex', 'offense', 'matchId']\n",
    "\n",
    "for col in cate_cols:\n",
    "    table_id = pp_table_id + f'.{col}_index_table'\n",
    "    # client.delete_table(table_id, not_found_ok=True)\n",
    "    # client.create_table(table_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_batch = f\"\"\"\n",
    "        CREATE TABLE `teemo-415918.match_dataset.match_test2_backup` AS\n",
    "        SELECT *\n",
    "        FROM `teemo-415918.match_dataset.match_test2`\n",
    "        \"\"\"\n",
    "\n",
    "client.query(query_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
